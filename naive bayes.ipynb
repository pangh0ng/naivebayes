{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们构建朴素贝叶斯模型进行垃圾邮件分类的步骤如下：\n",
    "1. 创建词典，使用已经处理过的数据集Ling-spam\n",
    "2. 提取特征\n",
    "3. 训练分类器\n",
    "4. 测试分类器效果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建词典\n",
    "我们打开数据集中的一封样本邮件可以得到如下信件内容："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: re : 2 . 882 s - > np np\n",
      "\n",
      "> deat : sun , 15 dec 91 2 : 25 : 2 est > : michael < mmorse @ vm1 . yorku . ca > > subject : re : 2 . 864 query > > wlodek zadrozny ask \" anything interest \" > construction \" s > np np \" . . . second , > much relate : consider construction form > discuss list late reduplication ? > logical sense \" john mcnamara name \" tautologous thus , > level , indistinguishable \" , , here ? \" . ' john mcnamara name ' tautologous support those logic-base semantics irrelevant natural language . sense tautologous ? supplies value attribute follow attribute value . fact value name-attribute relevant entity ' chaim shmendrik ' , ' john mcnamara name ' false . tautology , . ( reduplication , either . )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_email = open(\"./lingspam_public/lemm_stop/part1/3-1msg1.txt\",\"r\")\n",
    "print(sample_email.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: s - > np + np\n",
      "\n",
      "discussion s - > np + np remind ago read , source forget , critique newsmagazine ' unique tendency write style , most writer overly \" cute \" . one item tersely put down follow : \" 's favorite : colon . \" - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - lee hartman ga5123 @ siucvmb . bitnet department foreign language southern illinoi university carbondale , il 62901 u . s . .\n",
      "\n",
      "Subject: s - > np + np\n",
      "\n",
      "the discussion of s - > np + np reminds me that some years ago i read , in a source now forgotten , a critique of some newsmagazines ' unique tendencies in writing style , most of which the writer found overly \" cute \" . one item was tersely put down as follows : \" time 's favorite : the colon . \" - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - lee hartman ga5123 @ siucvmb . bitnet department of foreign languages southern illinois university carbondale , il 62901 u . s . a .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# python读文件的另一种方式\n",
    "with open('./lingspam_public/lemm_stop/part1/3-1msg2.txt') as f:\n",
    "    print(f.read())\n",
    "\n",
    "# 再读取一封没有经过文本清洗处理的信件\n",
    "with open('./lingspam_public/bare/part1/3-1msg2.txt') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面两封邮件可以看出，信件的内容是从第三行开始的。我们的第一步是建立一个词典，词典中单词和它的出现的频率一一映射。我们先读取出训练数据集中信件中的所有单词，然后使用Counter类计算出，每个词对应的次数，存入dictionary中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "def make_Dictionary(train_dir):\n",
    "    #emails = [os.path.join(train_dir,f) for f in os.listdir(train_dir)]    \n",
    "    emails = []\n",
    "    emails.append(train_dir)\n",
    "    all_words = []       \n",
    "    for mail in emails:    \n",
    "        with open(mail) as m:\n",
    "            for i,line in enumerate(m):\n",
    "                if i == 2:  #Body of email is only 3rd line of text file\n",
    "                    words = line.split()\n",
    "                    all_words += words\n",
    "     \n",
    "    dictionary = collections.Counter(all_words)\n",
    "    \n",
    "    # clear word that is not alpha, like numbers\n",
    "    # and clear word length = 1, like '>'\n",
    "    # make the frequency of that words to be zero\n",
    "    list_to_remove = dictionary.keys()\n",
    "    for item in list_to_remove:\n",
    "        if item.isalpha() == False: \n",
    "            dictionary[item]=0\n",
    "        elif len(item) == 1:\n",
    "            dictionary[item]=0\n",
    "    # end of clear\n",
    "    # choose the words which frequecy > 2 as feature\n",
    "    new_dict = []\n",
    "    for word,freq in dictionary.items():\n",
    "        if freq > 2:\n",
    "            new_dict.append({word,freq})\n",
    "    print(new_dict)\n",
    "    #dictionary = dictionary.most_common(3)\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{3, 'john'}, {3, 'mcnamara'}, {3, 'name'}, {'tautologous', 3}, {'value', 3}]\n",
      "Counter({'john': 3, 'mcnamara': 3, 'name': 3, 'tautologous': 3, 'value': 3, 'construction': 2, 'np': 2, 'reduplication': 2, 'sense': 2, 'attribute': 2, 'deat': 1, 'sun': 1, 'dec': 1, 'est': 1, 'michael': 1, 'mmorse': 1, 'yorku': 1, 'ca': 1, 'subject': 1, 're': 1, 'query': 1, 'wlodek': 1, 'zadrozny': 1, 'ask': 1, 'anything': 1, 'interest': 1, 'second': 1, 'much': 1, 'relate': 1, 'consider': 1, 'form': 1, 'discuss': 1, 'list': 1, 'late': 1, 'logical': 1, 'thus': 1, 'level': 1, 'indistinguishable': 1, 'here': 1, 'support': 1, 'those': 1, 'semantics': 1, 'irrelevant': 1, 'natural': 1, 'language': 1, 'supplies': 1, 'follow': 1, 'fact': 1, 'relevant': 1, 'entity': 1, 'chaim': 1, 'shmendrik': 1, 'false': 1, 'tautology': 1, 'either': 1, '>': 0, ':': 0, ',': 0, '15': 0, '91': 0, '2': 0, '25': 0, '<': 0, '@': 0, 'vm1': 0, '.': 0, '864': 0, '\"': 0, 's': 0, '?': 0, \"'\": 0, 'logic-base': 0, 'name-attribute': 0, '(': 0, ')': 0})\n"
     ]
    }
   ],
   "source": [
    "# 测试make_Dictionary函数\n",
    "# 针对一封信进行测试\n",
    "print(make_Dictionary('./lingspam_public/lemm_stop/part1/3-1msg1.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推广一封信至整个数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "def make_Dictionary(train_dir):\n",
    "    emails = [os.path.join(train_dir,f) for f in os.listdir(train_dir)]    \n",
    "    all_words = []       \n",
    "    for mail in emails:    \n",
    "        with open(mail) as m:\n",
    "            for i,line in enumerate(m):\n",
    "                if i == 2:  #Body of email is only 3rd line of text file\n",
    "                    words = line.split()\n",
    "                    all_words += words\n",
    "     \n",
    "    dictionary = collections.Counter(all_words)\n",
    "    \n",
    "    # clear word that is not alpha, like numbers\n",
    "    # and clear word length = 1, like '>'\n",
    "    # make the frequency of that words to be zero\n",
    "    list_to_remove = dictionary.keys()\n",
    "    for item in list_to_remove:\n",
    "        if item.isalpha() == False: \n",
    "            dictionary[item]=0\n",
    "        elif len(item) == 1:\n",
    "            dictionary[item]=0\n",
    "    # end of clear\n",
    "    # choose the words which frequecy > 20 as feature\n",
    "    # copy new_dict\n",
    "    new_dict = dictionary.copy()\n",
    "    # delete the freq < 20 in new dict\n",
    "    for word,freq in dictionary.items():\n",
    "        if freq < 20:\n",
    "            del new_dict[word]\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546\n",
      "[('language', 520), ('university', 296), ('one', 290), ('de', 253), ('linguistic', 234), ('work', 232), ('email', 216), ('information', 204), ('order', 203), ('address', 200)]\n"
     ]
    }
   ],
   "source": [
    "# 对目录'./lingspam_public/lemm_stop/part1/'中的所有信件建立字典\n",
    "# 打印字典的大小\n",
    "# 打印出前十个高频词\n",
    "dictionary = make_Dictionary('./lingspam_public/lemm_stop/part1/')\n",
    "print(len(dictionary))\n",
    "print(dictionary.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由上可知，我们的高频词有546个。我们用这些词作为特征计算出训练集中每一封信件的单词数向量，这个向量有546维。通常一封邮件的词频向量大部分维度都可能为零。\n",
    "我们要生成一个包含所有训练集中邮件的特征矩阵，矩阵的行数表示邮件数，列数表示特征向量长度。矩阵的值$M_i,j$表示，第i封信件中是否出现了第j个敏感词，值为一表示出现，值为零表示不出现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def extract_features(mail_dir,dictionary): \n",
    "    files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]\n",
    "    features_matrix = np.zeros((len(files),len(dictionary)))\n",
    "    print(len(files))\n",
    "    docID = 0;\n",
    "    for fil in files:\n",
    "      with open(fil) as fi:\n",
    "        for i,line in enumerate(fi):\n",
    "          if i == 2:\n",
    "            words = line.split()\n",
    "            for word in words:\n",
    "              wordID = 0\n",
    "              for i,d in enumerate(dictionary):\n",
    "                if d[0] == word:\n",
    "                  wordID = i\n",
    "                  features_matrix[docID,wordID] = 1\n",
    "                  #features_matrix[docID,wordID] = words.count(word)\n",
    "        docID = docID + 1    \n",
    "    return features_matrix\n",
    "\n",
    "matrix = extract_features('./lingspam_public/lemm_stop/part1/',dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182.0\n",
      "(289, 546)\n",
      "(3, 546)\n"
     ]
    }
   ],
   "source": [
    "# 打印出倒数第二份信的词向量\n",
    "#print(matrix[-2])\n",
    "# 数一数有多少个高频次在这封信中存在\n",
    "print(matrix[-2].sum(axis=0))\n",
    "# 竟然有182个高频词在这封信中出现\n",
    "print(matrix.shape)\n",
    "print(matrix[:3].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型参数\n",
    "从训练集的数据可以知道，训练集每封邮件的标签，在'./lingspam_public/lemm_stop/part1/'中有289封邮件，其中241封为正常邮件，48封为垃圾邮件，所以我们可以定义训练集的标签数组，前241为0，后48为1，其中0表示正常邮件，1表示垃圾邮件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 还是用现成的模型吧，自己写的模型有问题\n",
    "train_labels = np.zeros(289)\n",
    "train_labels[-48:]=1\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "model = MultinomialNB()\n",
    "model.fit(matrix, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(289,)\n",
      "48.0\n",
      "289\n",
      "[0.83391003 0.16608997]\n"
     ]
    }
   ],
   "source": [
    "train_labels = np.zeros(289)\n",
    "train_labels[-48:]=1\n",
    "print(train_labels.shape)\n",
    "n_1=train_labels.sum(axis=0)\n",
    "print(n_1)\n",
    "n=len(train_labels)\n",
    "print(n)\n",
    "P_Y = np.zeros(2)\n",
    "P_Y[1] = n_1/n\n",
    "P_Y[0] = 1 - P_Y[1]\n",
    "print(P_Y)\n",
    "# 获得了P(Y)的先验概率，Y就是本例中的邮件类别C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 没有自己没有做出来，感觉不对\n",
    "# 如果计算最大后验概率，直接计算与取对数计算，感觉数值有点奇怪\n",
    "# 没有继续往下写了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "求条件概率$P(X\\mid Y=是垃圾邮件)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(546,)\n",
      "[0.02967359 0.06824926 0.0148368  0.00890208 0.00296736 0.00296736\n",
      " 0.00296736 0.02967359 0.06824926]\n",
      "[0.97032641 0.93175074 0.9851632  0.99109792 0.99703264 0.99703264\n",
      " 0.99703264 0.97032641 0.93175074]\n",
      "[0.07358491 0.09622642 0.03962264 0.00377358 0.00188679 0.00188679\n",
      " 0.00188679 0.07358491 0.09622642]\n",
      "[0.92641509 0.90377358 0.96037736 0.99622642 0.99811321 0.99811321\n",
      " 0.99811321 0.92641509 0.90377358]\n"
     ]
    }
   ],
   "source": [
    "print(matrix[:241].sum(axis=0).shape)\n",
    "# 再求条件概率P(X|Y) \n",
    "k = len(dictionary)\n",
    "P_X_Y = np.zeros((2,k))\n",
    "# 当邮件为垃圾邮件时，高频词在信中存在时的条件概率估计\n",
    "P_X_Y[1] = (matrix[-48:].sum(axis=0)+1)/(n_1+n)\n",
    "# 当邮件为垃圾邮件时，高频词不在信中存在时的条件概率\n",
    "P_X_Y[0] = 1-P_X_Y[1]\n",
    "\n",
    "P_X_Y_0 = np.zeros((2,k))\n",
    "# 当邮件为正常时，高频词在信中存在的条件概率估计\n",
    "P_X_Y_0[1] = (matrix[:241].sum(axis=0)+1)/(n-n_1+n)\n",
    "# 当邮件为正常邮件时，高频词不在信中存在时的条件概率\n",
    "P_X_Y_0[0] = 1-P_X_Y_0[1]\n",
    "# 打印当邮件为垃圾邮件时，前10个高频词在信中存在时的条件概率\n",
    "print(P_X_Y[1,1:10])\n",
    "# 当邮件为垃圾邮件时，高频词不在信中的概率\n",
    "print(P_X_Y[0,1:10])\n",
    "# 打印当邮件为正常邮件时，前10个高频词在信中存在时的条件概率\n",
    "print(P_X_Y_0[1,1:10])\n",
    "# 当邮件为正常时，高频词不在信中的概率\n",
    "print(P_X_Y_0[0,1:10])\n",
    "# 预测当邮件正常时，高频词在信中存在的概率比邮件为垃圾邮件时的概率要低"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 546)\n",
      "29.166037735849056\n"
     ]
    }
   ],
   "source": [
    "# test 条件概率和\n",
    "print(P_X_Y_0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.sum(axis=0)表示按列的方向求和"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用模型进行分类\n",
    "模型的两个参数已经估计完成，使用测试集对模型进行测试。\n",
    "我们选用'./lingspam_public/lemm_stop/part10/'的所有信件作为测试数据集。我们已经知道测试数据集中有291封信，其中垃圾邮件49封，正常邮件242封。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291\n",
      "0.0\n",
      "(291, 546)\n",
      "(3, 546)\n"
     ]
    }
   ],
   "source": [
    "# 提取测试数据集中的特征词频向量组成的矩阵\n",
    "# 词典还是使用测试集中找出的高频词\n",
    "dictionary = make_Dictionary('./lingspam_public/lemm_stop/part1/')\n",
    "test_matrix = extract_features('./lingspam_public/lemm_stop/part10/',dictionary)\n",
    "# 打印出倒数第二份信的词向量\n",
    "#print(matrix[-2])\n",
    "# 数一数有多少个高频次在这封信中存在\n",
    "print(test_matrix[-2].sum(axis=0))\n",
    "# 竟然有182个高频词在这封信中出现\n",
    "print(test_matrix.shape)\n",
    "print(test_matrix[:3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.0\n"
     ]
    }
   ],
   "source": [
    "# 数一数有多少个高频次在这封信中存在\n",
    "print(test_matrix[-34].sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 1. 1. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# 使用贝叶斯公式求解，给定一封信件，输出它的概率\n",
    "# 对测试集特征矩阵求反，可以方便点乘计算\n",
    "# 求反，将其中为1的变为0，为0的变为1\n",
    "_test_matrix = abs(test_matrix-1)\n",
    "print(_test_matrix[-1,1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 求邮件是垃圾邮件的概率\n",
    "A = np.dot(_test_matrix, np.log(P_X_Y[0].T))\\\n",
    "+np.dot(test_matrix, np.log(P_X_Y[1].T))+np.log(P_Y[1])\n",
    "B = np.dot(_test_matrix, np.log(P_X_Y_0[0].T))\\\n",
    "+np.dot(test_matrix, np.log(P_X_Y_0[1].T))+np.log(P_Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(291,)\n",
      "1.3023263119408707e-43\n",
      "5.218506424368712e-41\n"
     ]
    }
   ],
   "source": [
    "print(A.shape)\n",
    "print(np.exp(A[-34]))\n",
    "print(np.exp(B[-34]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:1: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "D:\\Software\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:1: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "D:\\Software\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in greater\n",
      "  \n",
      "D:\\Software\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in less_equal\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\Software\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in greater\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "C = np.exp(B)/np.exp(A)\n",
    "C[C>3]=1\n",
    "C[C<=3]=0\n",
    "print(len(C[C>3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "213"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我们已经知道测试数据集中有291封信，其中垃圾邮件49封，正常邮件242封。\n",
    "test_labels=np.zeros(291)\n",
    "test_labels[-49:]=1\n",
    "D=test_labels-C\n",
    "len(D[D==0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:1: RuntimeWarning: invalid value encountered in greater\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(D[D>0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:1: RuntimeWarning: invalid value encountered in less\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(D[D<0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
