{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们构建朴素贝叶斯模型进行垃圾邮件分类的步骤如下：\n",
    "1. 创建词典，使用已经处理过的数据集Ling-spam\n",
    "2. 提取特征\n",
    "3. 训练分类器\n",
    "4. 测试分类器效果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建词典\n",
    "我们打开数据集中的一封样本邮件可以得到如下信件内容："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: re : 2 . 882 s - > np np\n",
      "\n",
      "> deat : sun , 15 dec 91 2 : 25 : 2 est > : michael < mmorse @ vm1 . yorku . ca > > subject : re : 2 . 864 query > > wlodek zadrozny ask \" anything interest \" > construction \" s > np np \" . . . second , > much relate : consider construction form > discuss list late reduplication ? > logical sense \" john mcnamara name \" tautologous thus , > level , indistinguishable \" , , here ? \" . ' john mcnamara name ' tautologous support those logic-base semantics irrelevant natural language . sense tautologous ? supplies value attribute follow attribute value . fact value name-attribute relevant entity ' chaim shmendrik ' , ' john mcnamara name ' false . tautology , . ( reduplication , either . )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_email = open(\"./lingspam_public/lemm_stop/part1/3-1msg1.txt\",\"r\")\n",
    "print(sample_email.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: s - > np + np\n",
      "\n",
      "discussion s - > np + np remind ago read , source forget , critique newsmagazine ' unique tendency write style , most writer overly \" cute \" . one item tersely put down follow : \" 's favorite : colon . \" - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - lee hartman ga5123 @ siucvmb . bitnet department foreign language southern illinoi university carbondale , il 62901 u . s . .\n",
      "\n",
      "Subject: s - > np + np\n",
      "\n",
      "the discussion of s - > np + np reminds me that some years ago i read , in a source now forgotten , a critique of some newsmagazines ' unique tendencies in writing style , most of which the writer found overly \" cute \" . one item was tersely put down as follows : \" time 's favorite : the colon . \" - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - lee hartman ga5123 @ siucvmb . bitnet department of foreign languages southern illinois university carbondale , il 62901 u . s . a .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# python读文件的另一种方式\n",
    "with open('./lingspam_public/lemm_stop/part1/3-1msg2.txt') as f:\n",
    "    print(f.read())\n",
    "\n",
    "# 再读取一封没有经过文本清洗处理的信件\n",
    "with open('./lingspam_public/bare/part1/3-1msg2.txt') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面两封邮件可以看出，信件的内容是从第三行开始的。我们的第一步是建立一个词典，词典中单词和它的出现的频率一一映射。我们先读取出训练数据集中信件中的所有单词，然后使用Counter类计算出，每个词对应的次数，存入dictionary中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "def make_Dictionary(train_dir):\n",
    "    #emails = [os.path.join(train_dir,f) for f in os.listdir(train_dir)]    \n",
    "    emails = []\n",
    "    emails.append(train_dir)\n",
    "    all_words = []       \n",
    "    for mail in emails:    \n",
    "        with open(mail) as m:\n",
    "            for i,line in enumerate(m):\n",
    "                if i == 2:  #Body of email is only 3rd line of text file\n",
    "                    words = line.split()\n",
    "                    all_words += words\n",
    "     \n",
    "    dictionary = collections.Counter(all_words)\n",
    "    \n",
    "    # clear word that is not alpha, like numbers\n",
    "    # and clear word length = 1, like '>'\n",
    "    # make the frequency of that words to be zero\n",
    "    list_to_remove = dictionary.keys()\n",
    "    for item in list_to_remove:\n",
    "        if item.isalpha() == False: \n",
    "            dictionary[item]=0\n",
    "        elif len(item) == 1:\n",
    "            dictionary[item]=0\n",
    "    # end of clear\n",
    "    # choose the words which frequecy > 2 as feature\n",
    "    new_dict = []\n",
    "    for word,freq in dictionary.items():\n",
    "        if freq > 2:\n",
    "            new_dict.append({word,freq})\n",
    "    print(new_dict)\n",
    "    #dictionary = dictionary.most_common(3)\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{3, 'john'}, {3, 'mcnamara'}, {3, 'name'}, {'tautologous', 3}, {'value', 3}]\n",
      "Counter({'john': 3, 'mcnamara': 3, 'name': 3, 'tautologous': 3, 'value': 3, 'construction': 2, 'np': 2, 'reduplication': 2, 'sense': 2, 'attribute': 2, 'deat': 1, 'sun': 1, 'dec': 1, 'est': 1, 'michael': 1, 'mmorse': 1, 'yorku': 1, 'ca': 1, 'subject': 1, 're': 1, 'query': 1, 'wlodek': 1, 'zadrozny': 1, 'ask': 1, 'anything': 1, 'interest': 1, 'second': 1, 'much': 1, 'relate': 1, 'consider': 1, 'form': 1, 'discuss': 1, 'list': 1, 'late': 1, 'logical': 1, 'thus': 1, 'level': 1, 'indistinguishable': 1, 'here': 1, 'support': 1, 'those': 1, 'semantics': 1, 'irrelevant': 1, 'natural': 1, 'language': 1, 'supplies': 1, 'follow': 1, 'fact': 1, 'relevant': 1, 'entity': 1, 'chaim': 1, 'shmendrik': 1, 'false': 1, 'tautology': 1, 'either': 1, '>': 0, ':': 0, ',': 0, '15': 0, '91': 0, '2': 0, '25': 0, '<': 0, '@': 0, 'vm1': 0, '.': 0, '864': 0, '\"': 0, 's': 0, '?': 0, \"'\": 0, 'logic-base': 0, 'name-attribute': 0, '(': 0, ')': 0})\n"
     ]
    }
   ],
   "source": [
    "# 测试make_Dictionary函数\n",
    "# 针对一封信进行测试\n",
    "print(make_Dictionary('./lingspam_public/lemm_stop/part1/3-1msg1.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推广一封信至整个数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "def make_Dictionary(train_dir):\n",
    "    emails = [os.path.join(train_dir,f) for f in os.listdir(train_dir)]    \n",
    "    all_words = []       \n",
    "    for mail in emails:    \n",
    "        with open(mail) as m:\n",
    "            for i,line in enumerate(m):\n",
    "                if i == 2:  #Body of email is only 3rd line of text file\n",
    "                    words = line.split()\n",
    "                    all_words += words\n",
    "     \n",
    "    dictionary = collections.Counter(all_words)\n",
    "    \n",
    "    # clear word that is not alpha, like numbers\n",
    "    # and clear word length = 1, like '>'\n",
    "    # make the frequency of that words to be zero\n",
    "    list_to_remove = dictionary.keys()\n",
    "    for item in list_to_remove:\n",
    "        if item.isalpha() == False: \n",
    "            dictionary[item]=0\n",
    "        elif len(item) == 1:\n",
    "            dictionary[item]=0\n",
    "    # end of clear\n",
    "    # choose the words which frequecy > 20 as feature\n",
    "    # copy new_dict\n",
    "    new_dict = dictionary.copy()\n",
    "    # delete the freq < 20 in new dict\n",
    "    for word,freq in dictionary.items():\n",
    "        if freq < 20:\n",
    "            del new_dict[word]\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546\n",
      "[('language', 520), ('university', 296), ('one', 290), ('de', 253), ('linguistic', 234), ('work', 232), ('email', 216), ('information', 204), ('order', 203), ('address', 200)]\n"
     ]
    }
   ],
   "source": [
    "# 对目录'./lingspam_public/lemm_stop/part1/'中的所有信件建立字典\n",
    "# 打印字典的大小\n",
    "# 打印出前十个高频词\n",
    "dictionary = make_Dictionary('./lingspam_public/lemm_stop/part1/')\n",
    "print(len(dictionary))\n",
    "# k表示我们的特征数\n",
    "k = len(dictionary)\n",
    "print(dictionary.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由上可知，我们的高频词有546个。我们用这些词作为特征计算出训练集中每一封信件的单词数向量，这个向量有546维。通常一封邮件的词频向量大部分维度都可能为零。\n",
    "我们要生成一个包含所有训练集中邮件的特征矩阵，矩阵的行数表示邮件数，列数表示特征向量长度。矩阵的值$m_{i,j}$表示，第i封信件中是否出现了第j个敏感词，$m_{i,j}=1$表示出现，$m_{i,j}=0$表示不出现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def extract_features(mail_dir,dictionary): \n",
    "    files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]\n",
    "    features_matrix = np.zeros((len(files),len(dictionary)))\n",
    "    print(len(files))\n",
    "    docID = 0;\n",
    "    for fil in files:\n",
    "      with open(fil) as fi:\n",
    "        for i,line in enumerate(fi):\n",
    "          if i == 2:\n",
    "            words = line.split()\n",
    "            for word in words:\n",
    "              wordID = 0\n",
    "              for i,d in enumerate(dictionary):\n",
    "                if d[0] == word:\n",
    "                  wordID = i\n",
    "                  features_matrix[docID,wordID] = 1\n",
    "                  #features_matrix[docID,wordID] = words.count(word)\n",
    "        docID = docID + 1    \n",
    "    return features_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289\n"
     ]
    }
   ],
   "source": [
    "matrix = extract_features('./lingspam_public/lemm_stop/part1/',dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178.0\n",
      "(289, 546)\n",
      "(3, 546)\n"
     ]
    }
   ],
   "source": [
    "# 打印出倒数第二份信的词向量\n",
    "#print(matrix[-2])\n",
    "# 数一数有多少个高频次在这封信中存在\n",
    "print(matrix[-2].sum(axis=0))\n",
    "# 竟然有182个高频词在这封信中出现\n",
    "print(matrix.shape)\n",
    "print(matrix[:3].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型参数\n",
    "从训练集的数据可以知道，训练集每封邮件的标签，在'./lingspam_public/lemm_stop/part1/'中有289封邮件，其中241封为正常邮件，48封为垃圾邮件，所以我们可以定义训练集的标签数组，前241为0，后48为1，其中0表示正常邮件，1表示垃圾邮件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(289,)\n",
      "48.0\n",
      "289\n",
      "0.16608996539792387\n"
     ]
    }
   ],
   "source": [
    "train_labels = np.zeros(289)\n",
    "train_labels[-48:]=1\n",
    "print(train_labels.shape)\n",
    "n_1=train_labels.sum(axis=0)\n",
    "print(n_1)\n",
    "n=len(train_labels)\n",
    "print(n)\n",
    "# 估计当Y是垃圾邮件时的概率=P_Y\n",
    "# 使用垃圾邮件的封数与总邮件数的比值\n",
    "P_Y = n_1/n\n",
    "print(P_Y)\n",
    "# 获得了P(Y)的先验概率，Y就是本例中的邮件类别C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "估计条件概率$P(X\\mid Y=垃圾邮件)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(546,)\n",
      "[0.01186944 0.02077151 0.00296736 0.01780415 0.02077151 0.00593472\n",
      " 0.00890208 0.03560831 0.04747774 0.01186944]\n"
     ]
    }
   ],
   "source": [
    "print(matrix[:241].sum(axis=0).shape)\n",
    "# 再求条件概率P(X|Y) \n",
    "k = len(dictionary)\n",
    "P_X_Y = np.zeros(k)\n",
    "#P_X_Y = np.zeros((2,k))\n",
    "# 当邮件为垃圾邮件时，特征词在信中存在时的条件概率估计\n",
    "# P(X_i=1|Y=1)表示为P_X_Y[1]\n",
    "# 特征词在垃圾邮件中出现的封数与总的垃圾邮件数的比值\n",
    "# 为了避免零概率问题，我们使用Laplace smoothing\n",
    "# 即特征词在垃圾邮件中出现的封数加一后与垃圾邮件加总邮件数的比值来估计\n",
    "P_X_Y = (matrix[-48:].sum(axis=0)+1)/(n_1+n)\n",
    "#P_X_Y[1] = (matrix[-48:].sum(axis=0)+1)/(n_1+n)\n",
    "# 当邮件为正常邮件时，高频词在信中存在时的条件概率\n",
    "# P_X_Y[0]表示P(X_i=1|Y=0)\n",
    "# 特征词在正常邮件中出现的封数加一与正常邮件加总邮件数的比值\n",
    "#P_X_Y[0] = (matrix[:241].sum(axis=0)+1)/(n-n_1+n)\n",
    "\n",
    "# 打印当邮件为垃圾邮件时，前10个高频词在信中存在时的条件概率\n",
    "print(P_X_Y[-10:])\n",
    "#print(P_X_Y[1,1:10])\n",
    "# 当邮件为垃圾邮件时，高频词不在信中的概率\n",
    "#print(P_X_Y[0,1:10])\n",
    "# 预测当邮件正常时，高频词在信中存在的概率比邮件为垃圾邮件时的概率要低"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(546,)\n"
     ]
    }
   ],
   "source": [
    "# test 条件概率和\n",
    "print(P_X_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.sum(axis=0)表示按列的方向求和"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用模型进行分类\n",
    "模型的两个参数已经估计完成，使用测试集对模型进行测试。\n",
    "我们选用'./lingspam_public/lemm_stop/part9/'的所有信件作为测试数据集。我们已经知道测试数据集中有289封信，其中垃圾邮件48封，正常邮件241封。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289\n",
      "90.0\n",
      "(289, 546)\n"
     ]
    }
   ],
   "source": [
    "# 提取测试数据集中的特征词频向量组成的矩阵\n",
    "# 词典还是使用测试集中找出的高频词\n",
    "dictionary = make_Dictionary('./lingspam_public/lemm_stop/part1/')\n",
    "test_matrix = extract_features('./lingspam_public/lemm_stop/part9/',dictionary)\n",
    "# 打印出倒数第二份信的词向量\n",
    "#print(matrix[-2])\n",
    "# 数一数有多少个高频次在这封信中存在\n",
    "print(test_matrix[-2].sum(axis=0))\n",
    "# 竟然有182个高频词在这封信中出现\n",
    "print(test_matrix.shape)\n",
    "test_num=289\n",
    "test_spam=48\n",
    "test_ham=241"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.879729836263757e-186\n"
     ]
    }
   ],
   "source": [
    "# 邮件正常时，特征存在的条件概率\n",
    "P_X_Y_0 = (matrix[:241].sum(axis=0)+1)/(n-n_1+n)\n",
    "b=1-P_Y\n",
    "i=0\n",
    "for p in P_X_Y_0:\n",
    "    if test_matrix[4][i]==0:\n",
    "        b*=1-p\n",
    "    else:\n",
    "        b*=p\n",
    "    i=i+1\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(291,)\n",
      "(546,)\n",
      "0.0\n",
      "1.2163746224314662e-264\n"
     ]
    }
   ],
   "source": [
    "# 预测一封信为垃圾邮件的概率\n",
    "# P(Y|X)=P(Y)P(X|Y)\n",
    "P_Y_X = np.ones(291)*P_Y\n",
    "print(P_Y_X.shape)\n",
    "print(P_X_Y.shape)\n",
    "print(test_matrix[0][0])\n",
    "i=0\n",
    "a=P_Y\n",
    "for p in P_X_Y:\n",
    "    if test_matrix[4][i]==0:\n",
    "        a*=1-p\n",
    "    else:\n",
    "        a*=p\n",
    "    i=i+1\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_Y_X = np.ones(test_num)*P_Y\n",
    "P_Y_0_X = np.ones(test_num)*(1-P_Y)\n",
    "predict = np.zeros(test_num)\n",
    "j=0\n",
    "for mail in test_matrix:\n",
    "    i=0\n",
    "    for p in P_X_Y:\n",
    "        if mail[i]==0:\n",
    "            P_Y_X[j]*=1-p\n",
    "        else:\n",
    "            P_Y_X[j]*=p\n",
    "        i=i+1\n",
    "    i=0\n",
    "    for p in P_X_Y_0:\n",
    "        if mail[i]==0:\n",
    "            P_Y_0_X[j]*=1-p\n",
    "        else:\n",
    "            P_Y_0_X[j]*=p\n",
    "        i=i+1\n",
    "    if P_Y_X[j] > P_Y_0_X[j]:\n",
    "        predict[j]=1\n",
    "    j=j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 0. 0. 0. 0. 1. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(predict[-20:-10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[169, 31], [72, 17]]\n",
      "0.643598615916955\n"
     ]
    }
   ],
   "source": [
    "# 我们已经知道测试数据集中有291封信，其中垃圾邮件49封，正常邮件242封。\n",
    "test_labels=np.zeros(test_num)\n",
    "test_labels[-test_spam:]=1\n",
    "a = test_labels-predict\n",
    "b = test_labels+predict\n",
    "con_matrix=[[len(b[b==0]),len(a[a==1])],\n",
    "           [len(a[a==-1]),len(b[b==2])]]\n",
    "print(con_matrix)\n",
    "print(len(a[a==0])/test_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "针对同一个测试数据集，我自己写的模型正确率为64%，但是调用sklearn库模型的正确率为79%，这个说明我的模型还可以提高。我的模型和sklearn的模型有什么差异呢？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
